{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "from models import Transformer\n",
    "import dataloader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import  DataLoader, random_split\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('AGG')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 42323\n",
      "Length of training dataset: 33858\n",
      "Length of validation dataset: 8465\n"
     ]
    }
   ],
   "source": [
    "window_size = 10\n",
    "batch_size= 1024\n",
    "learning_rate= 0.0001\n",
    "num_epochs= 300\n",
    "input_size= 1\n",
    "\n",
    "path_train = 'dataset/structured_logs/train_data'\n",
    "vocab2idx, log_dict = dataloader.to_idx(path_train)\n",
    "\n",
    "vocab_sz = len(vocab2idx)\n",
    "\n",
    "data = dataloader.generate_data_for_training(vocab2idx, log_dict, window_size)\n",
    "\n",
    "train_size = int(len(data) * 0.8)\n",
    "validate_size = len(data) - train_size\n",
    "print(\"Length of dataset: {}\".format(len(data)))\n",
    "print(\"Length of training dataset: {}\".format(train_size))\n",
    "print(\"Length of validation dataset: {}\".format(validate_size))\n",
    "\n",
    "train_dataset, val_dataset = random_split(data, [train_size, validate_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size= batch_size, shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "        in_dim= input_size,\n",
    "        embed_dim= 64, \n",
    "        out_dim= vocab_sz,\n",
    "        window_size= window_size,\n",
    "        depth= 6,\n",
    "        heads= 8,\n",
    "        dim_head= 64,\n",
    "        dim_ratio= 2,\n",
    "        dropout= 0.1\n",
    "    )\n",
    "\n",
    "model = nn.DataParallel(model) # multi-GPU\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "\n",
    "# Train the model\n",
    "loss_min = 99999\n",
    "model_name = 'best_model.pth'\n",
    "model_path = \"saved_models\"\n",
    "\n",
    "save_path = os.path.join(model_path,model_name)\n",
    "best_model = model\n",
    "train_loss_list = []\n",
    "val_loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training ......\n",
      "Epoch [2/300], train_loss: 0.00956063642961 val loss: 0.00901817116605\n",
      "Model saved\n",
      "Epoch [3/300], train_loss: 0.00965904713923 val loss: 0.00783411410197\n",
      "Model saved\n",
      "Epoch [4/300], train_loss: 0.00695005051025 val loss: 0.00637991370079\n",
      "Epoch [5/300], train_loss: 0.00672087201549 val loss: 0.00791823140266\n",
      "Epoch [6/300], train_loss: 0.00602535025717 val loss: 0.00644872131266\n",
      "Model saved\n",
      "Epoch [7/300], train_loss: 0.00559460896843 val loss: 0.00619082721864\n",
      "Epoch [8/300], train_loss: 0.00553573681014 val loss: 0.00781002995144\n",
      "Epoch [9/300], train_loss: 0.00581605177799 val loss: 0.01069821439483\n",
      "Model saved\n",
      "Epoch [10/300], train_loss: 0.00589074199439 val loss: 0.00605714093803\n",
      "Epoch [11/300], train_loss: 0.00511163687347 val loss: 0.00697922125700\n",
      "Epoch [12/300], train_loss: 0.00548978195301 val loss: 0.00640616353970\n",
      "Epoch [13/300], train_loss: 0.00497604420011 val loss: 0.00861442180920\n",
      "Epoch [14/300], train_loss: 0.00521388845792 val loss: 0.00700374285872\n",
      "Epoch [15/300], train_loss: 0.00549189328654 val loss: 0.00653572579096\n",
      "Epoch [16/300], train_loss: 0.00561186488759 val loss: 0.00622954722106\n",
      "Epoch [17/300], train_loss: 0.00509719919886 val loss: 0.00715236825312\n",
      "Epoch [18/300], train_loss: 0.00541315737235 val loss: 0.00641846268587\n",
      "Epoch [19/300], train_loss: 0.00965421920409 val loss: 0.00792482732019\n",
      "Epoch [20/300], train_loss: 0.00821533814451 val loss: 0.00816044600085\n",
      "Epoch [21/300], train_loss: 0.00514356279232 val loss: 0.00622443872918\n",
      "Epoch [22/300], train_loss: 0.00556833780737 val loss: 0.00660033247227\n",
      "Epoch [23/300], train_loss: 0.00528131203550 val loss: 0.00755533687253\n",
      "Epoch [24/300], train_loss: 0.00524756028472 val loss: 0.00790562431474\n",
      "Epoch [25/300], train_loss: 0.00519822213477 val loss: 0.00822499704211\n",
      "Epoch [26/300], train_loss: 0.00484345406157 val loss: 0.00887072291355\n",
      "Epoch [27/300], train_loss: 0.00462267525462 val loss: 0.00837368175902\n",
      "Epoch [28/300], train_loss: 0.00507290179923 val loss: 0.00784114186536\n",
      "Epoch [29/300], train_loss: 0.00486790739952 val loss: 0.00650514721767\n",
      "Epoch [30/300], train_loss: 0.00527426550295 val loss: 0.00799228677190\n",
      "Epoch [31/300], train_loss: 0.00490314107190 val loss: 0.00701633251608\n",
      "Epoch [32/300], train_loss: 0.00463760982144 val loss: 0.00611326216798\n",
      "Epoch [33/300], train_loss: 0.00449583980833 val loss: 0.00721844211673\n",
      "Epoch [34/300], train_loss: 0.00486681099146 val loss: 0.00853962968621\n",
      "Epoch [35/300], train_loss: 0.00446590588784 val loss: 0.00767576779860\n",
      "Epoch [36/300], train_loss: 0.00476998030996 val loss: 0.00778411827762\n",
      "Epoch [37/300], train_loss: 0.00422519851161 val loss: 0.00776030952855\n",
      "Epoch [38/300], train_loss: 0.00478832298027 val loss: 0.00695070355950\n",
      "Epoch [39/300], train_loss: 0.00421903308084 val loss: 0.00889807972352\n",
      "Epoch [40/300], train_loss: 0.00538974332791 val loss: 0.00781817653559\n",
      "Epoch [41/300], train_loss: 0.00550450787277 val loss: 0.00750889392334\n",
      "Epoch [42/300], train_loss: 0.00562144056740 val loss: 0.00711498037951\n",
      "Epoch [43/300], train_loss: 0.00831677707474 val loss: 0.01038098542227\n",
      "Epoch [44/300], train_loss: 0.00793232837938 val loss: 0.00979152072169\n",
      "Epoch [45/300], train_loss: 0.00522998983926 val loss: 0.00810785028928\n",
      "Epoch [46/300], train_loss: 0.00569932998684 val loss: 0.00710051182493\n",
      "Epoch [47/300], train_loss: 0.00497325461680 val loss: 0.00966645220372\n",
      "Epoch [48/300], train_loss: 0.00464179289659 val loss: 0.00959845493748\n",
      "Epoch [49/300], train_loss: 0.00479006274562 val loss: 0.00759109333416\n",
      "Epoch [50/300], train_loss: 0.00444305642604 val loss: 0.00742753403028\n",
      "Epoch [51/300], train_loss: 0.00517463526408 val loss: 0.00757437447060\n",
      "Epoch [52/300], train_loss: 0.00478821954354 val loss: 0.00737008521941\n",
      "Epoch [53/300], train_loss: 0.00454302085628 val loss: 0.00883088885247\n",
      "Epoch [54/300], train_loss: 0.00481474915207 val loss: 0.00901350117783\n",
      "Epoch [55/300], train_loss: 0.00495734587243 val loss: 0.00752019149109\n",
      "Epoch [56/300], train_loss: 0.00449575885089 val loss: 0.00887046907640\n",
      "Epoch [57/300], train_loss: 0.00577060086396 val loss: 0.00830934655662\n",
      "Epoch [58/300], train_loss: 0.00504200282542 val loss: 0.00816894139199\n",
      "Epoch [59/300], train_loss: 0.00460121811107 val loss: 0.00823881136360\n",
      "Epoch [60/300], train_loss: 0.00553693215239 val loss: 0.00840664819245\n",
      "Model saved\n",
      "Epoch [61/300], train_loss: 0.00518499803045 val loss: 0.00604717450268\n",
      "Epoch [62/300], train_loss: 0.00500052078036 val loss: 0.01043780085497\n",
      "Epoch [63/300], train_loss: 0.00535930337227 val loss: 0.00649366248399\n",
      "Epoch [64/300], train_loss: 0.00487270452119 val loss: 0.00756654722095\n",
      "Epoch [65/300], train_loss: 0.00512025203309 val loss: 0.00741072285907\n",
      "Epoch [66/300], train_loss: 0.00522547402515 val loss: 0.00795170701976\n",
      "Epoch [67/300], train_loss: 0.00452585196806 val loss: 0.01023884288588\n",
      "Epoch [68/300], train_loss: 0.00537998416899 val loss: 0.00685007339861\n",
      "Epoch [69/300], train_loss: 0.00503520697684 val loss: 0.00941480479216\n",
      "Epoch [70/300], train_loss: 0.00479422909078 val loss: 0.00732500607329\n",
      "Epoch [71/300], train_loss: 0.00472683982020 val loss: 0.00752864602772\n",
      "Epoch [72/300], train_loss: 0.00436887519112 val loss: 0.00820353570937\n",
      "Epoch [73/300], train_loss: 0.00437725330309 val loss: 0.00738836142813\n",
      "Epoch [74/300], train_loss: 0.00433237501805 val loss: 0.00730289545673\n",
      "Epoch [75/300], train_loss: 0.00391741023625 val loss: 0.00759691164623\n",
      "Epoch [76/300], train_loss: 0.00490286908291 val loss: 0.00855550371153\n",
      "Epoch [77/300], train_loss: 0.00501777970588 val loss: 0.00981437374139\n",
      "Epoch [78/300], train_loss: 0.00537175106515 val loss: 0.00730047779669\n",
      "Epoch [79/300], train_loss: 0.00527932918554 val loss: 0.00882317335345\n",
      "Epoch [80/300], train_loss: 0.00452628928949 val loss: 0.00975818342219\n",
      "Epoch [81/300], train_loss: 0.00440010244604 val loss: 0.01073774449631\n",
      "Epoch [82/300], train_loss: 0.00473856748945 val loss: 0.00951538622030\n",
      "Epoch [83/300], train_loss: 0.00549575617718 val loss: 0.00811258834437\n",
      "Epoch [84/300], train_loss: 0.00459219104446 val loss: 0.00907492065138\n",
      "Epoch [85/300], train_loss: 0.00458396633964 val loss: 0.00767586768941\n",
      "Epoch [86/300], train_loss: 0.00453039496557 val loss: 0.00774442414048\n",
      "Epoch [87/300], train_loss: 0.00446265699130 val loss: 0.00748273256856\n",
      "Epoch [88/300], train_loss: 0.00762227732989 val loss: 0.01011143976616\n",
      "Epoch [89/300], train_loss: 0.00898776462859 val loss: 0.01088328526950\n",
      "Epoch [90/300], train_loss: 0.00475102231867 val loss: 0.01069161914739\n",
      "Epoch [91/300], train_loss: 0.00475829981994 val loss: 0.01036966158022\n",
      "Epoch [92/300], train_loss: 0.00426272390734 val loss: 0.01239825787424\n",
      "Epoch [93/300], train_loss: 0.00461526753840 val loss: 0.00872381986119\n",
      "Epoch [94/300], train_loss: 0.00522393786848 val loss: 0.00898940299683\n",
      "Epoch [95/300], train_loss: 0.00547834703278 val loss: 0.01428287429331\n",
      "Epoch [96/300], train_loss: 0.00501051780028 val loss: 0.00927244954639\n",
      "Epoch [97/300], train_loss: 0.00512281973052 val loss: 0.00989666863138\n",
      "Epoch [98/300], train_loss: 0.00516741700053 val loss: 0.00747548184720\n",
      "Epoch [99/300], train_loss: 0.00455851245137 val loss: 0.00863385529050\n",
      "Epoch [100/300], train_loss: 0.00875149812440 val loss: 0.01094605701251\n",
      "Epoch [101/300], train_loss: 0.00584375568125 val loss: 0.00911874018978\n",
      "Epoch [102/300], train_loss: 0.00555736465739 val loss: 0.00956445651981\n",
      "Epoch [103/300], train_loss: 0.00480075800100 val loss: 0.00832528936088\n",
      "Epoch [104/300], train_loss: 0.00484478729092 val loss: 0.00714810118209\n",
      "Epoch [105/300], train_loss: 0.00535440161612 val loss: 0.01062454203040\n",
      "Epoch [106/300], train_loss: 0.00532954056004 val loss: 0.01050548772198\n",
      "Epoch [107/300], train_loss: 0.00538880736499 val loss: 0.00912921369829\n",
      "Epoch [108/300], train_loss: 0.00583619750892 val loss: 0.01021691543994\n",
      "Epoch [109/300], train_loss: 0.00474251329990 val loss: 0.00991723107937\n",
      "Epoch [110/300], train_loss: 0.00514362588964 val loss: 0.00852224072312\n",
      "Epoch [111/300], train_loss: 0.00445383186140 val loss: 0.01081640059460\n",
      "Epoch [112/300], train_loss: 0.00445833404442 val loss: 0.00924297683135\n",
      "Epoch [113/300], train_loss: 0.00475144503049 val loss: 0.00840019100825\n",
      "Epoch [114/300], train_loss: 0.00472852740067 val loss: 0.01093051979185\n",
      "Epoch [115/300], train_loss: 0.00476728245806 val loss: 0.00949996889297\n",
      "Epoch [116/300], train_loss: 0.00421001295763 val loss: 0.01082925687660\n",
      "Epoch [117/300], train_loss: 0.00434814034869 val loss: 0.00761069032807\n",
      "Epoch [118/300], train_loss: 0.00515886242005 val loss: 0.00765322378882\n",
      "Epoch [119/300], train_loss: 0.00511886025197 val loss: 0.00991308763494\n",
      "Epoch [120/300], train_loss: 0.00452460733461 val loss: 0.00884923537766\n",
      "Epoch [121/300], train_loss: 0.00420218413680 val loss: 0.01186180845091\n",
      "Epoch [122/300], train_loss: 0.00445213449455 val loss: 0.00771951309677\n",
      "Epoch [123/300], train_loss: 0.00441018813067 val loss: 0.00883833249746\n",
      "Epoch [124/300], train_loss: 0.00396130971518 val loss: 0.01046726471476\n",
      "Epoch [125/300], train_loss: 0.00416868561554 val loss: 0.01019408107903\n",
      "Epoch [126/300], train_loss: 0.00414796068799 val loss: 0.00869208685536\n",
      "Epoch [127/300], train_loss: 0.00445001881984 val loss: 0.00817911058160\n",
      "Epoch [128/300], train_loss: 0.00446377824632 val loss: 0.00879317801446\n",
      "Epoch [129/300], train_loss: 0.00422156747321 val loss: 0.00778708948443\n",
      "Epoch [130/300], train_loss: 0.00465755225570 val loss: 0.00942118992356\n",
      "Epoch [131/300], train_loss: 0.00424721901928 val loss: 0.00853455638409\n",
      "Epoch [132/300], train_loss: 0.00455599982567 val loss: 0.00852018470727\n",
      "Epoch [133/300], train_loss: 0.00443648547362 val loss: 0.00839678465531\n",
      "Epoch [134/300], train_loss: 0.00515956473057 val loss: 0.00791261540467\n",
      "Epoch [135/300], train_loss: 0.00450142851675 val loss: 0.00999851701716\n",
      "Epoch [136/300], train_loss: 0.00431097557542 val loss: 0.01143120649997\n",
      "Epoch [137/300], train_loss: 0.00535511207058 val loss: 0.00816571003977\n",
      "Epoch [138/300], train_loss: 0.00428542462160 val loss: 0.00678337814801\n",
      "Epoch [139/300], train_loss: 0.00460153547208 val loss: 0.00787169755656\n",
      "Epoch [140/300], train_loss: 0.00455969682138 val loss: 0.00754912122405\n",
      "Epoch [141/300], train_loss: 0.00468271396034 val loss: 0.00722531734977\n",
      "Epoch [142/300], train_loss: 0.00425284879234 val loss: 0.00824746379577\n",
      "Epoch [143/300], train_loss: 0.00400574438390 val loss: 0.00947556912191\n",
      "Epoch [144/300], train_loss: 0.00467639986207 val loss: 0.00755011422249\n",
      "Epoch [145/300], train_loss: 0.00445645464360 val loss: 0.00786073453006\n",
      "Epoch [146/300], train_loss: 0.00967183587422 val loss: 0.00930102697263\n",
      "Epoch [147/300], train_loss: 0.00638467372934 val loss: 0.01192890217084\n",
      "Epoch [148/300], train_loss: 0.00488375857313 val loss: 0.01094255567821\n",
      "Epoch [149/300], train_loss: 0.00468798801825 val loss: 0.00923513471045\n",
      "Epoch [150/300], train_loss: 0.00500227999360 val loss: 0.00824852664826\n",
      "Epoch [151/300], train_loss: 0.00459124903136 val loss: 0.00983282712857\n",
      "Epoch [152/300], train_loss: 0.00465312300281 val loss: 0.01111787476354\n",
      "Epoch [153/300], train_loss: 0.00587261584074 val loss: 0.01109953050359\n",
      "Epoch [154/300], train_loss: 0.00522616022328 val loss: 0.01331204721323\n",
      "Epoch [155/300], train_loss: 0.00561927307075 val loss: 0.01251853506902\n",
      "Epoch [156/300], train_loss: 0.00551631106218 val loss: 0.01377907384368\n",
      "Epoch [157/300], train_loss: 0.00564242297825 val loss: 0.01168977361198\n",
      "Epoch [158/300], train_loss: 0.00575425421737 val loss: 0.00938718392798\n",
      "Epoch [159/300], train_loss: 0.00581595459816 val loss: 0.01329200811354\n",
      "Epoch [160/300], train_loss: 0.00600672658851 val loss: 0.01101268336909\n",
      "Epoch [161/300], train_loss: 0.00603287244089 val loss: 0.01387592495060\n",
      "Epoch [162/300], train_loss: 0.00544361064053 val loss: 0.00988653231778\n",
      "Epoch [163/300], train_loss: 0.00490426003543 val loss: 0.01666504256233\n",
      "Epoch [164/300], train_loss: 0.00571881710237 val loss: 0.00936775041110\n",
      "Epoch [165/300], train_loss: 0.00510330487433 val loss: 0.00882574358871\n",
      "Epoch [166/300], train_loss: 0.00524492876310 val loss: 0.01097100084169\n",
      "Epoch [167/300], train_loss: 0.00461391020765 val loss: 0.00881838626325\n",
      "Epoch [168/300], train_loss: 0.00426295921224 val loss: 0.01162532778027\n",
      "Epoch [169/300], train_loss: 0.00497197542807 val loss: 0.01274468212937\n",
      "Epoch [170/300], train_loss: 0.00459559220540 val loss: 0.01025077146995\n",
      "Epoch [171/300], train_loss: 0.00453001305740 val loss: 0.01347816937778\n",
      "Epoch [172/300], train_loss: 0.00446375783598 val loss: 0.01125708616261\n",
      "Epoch [173/300], train_loss: 0.00437469707172 val loss: 0.01239147801405\n",
      "Epoch [174/300], train_loss: 0.00474570439581 val loss: 0.00940929625358\n",
      "Epoch [175/300], train_loss: 0.00456290339421 val loss: 0.01171658248414\n",
      "Epoch [176/300], train_loss: 0.00438310618184 val loss: 0.01100565890212\n",
      "Epoch [177/300], train_loss: 0.00483907838926 val loss: 0.00949310758976\n",
      "Epoch [178/300], train_loss: 0.00461983009882 val loss: 0.01584389171088\n",
      "Epoch [179/300], train_loss: 0.00455196865112 val loss: 0.01246659750662\n",
      "Epoch [180/300], train_loss: 0.00410868588391 val loss: 0.01298287197440\n",
      "Epoch [181/300], train_loss: 0.00444566759474 val loss: 0.01396578683812\n",
      "Epoch [182/300], train_loss: 0.00509938369984 val loss: 0.01025296847931\n",
      "Epoch [183/300], train_loss: 0.00466828457480 val loss: 0.00780965061616\n",
      "Epoch [184/300], train_loss: 0.00496282399742 val loss: 0.00799439795567\n",
      "Epoch [185/300], train_loss: 0.00455960833985 val loss: 0.00945293431384\n",
      "Epoch [186/300], train_loss: 0.00495649863767 val loss: 0.01433573034592\n",
      "Epoch [187/300], train_loss: 0.00518165875623 val loss: 0.01098197309895\n",
      "Epoch [188/300], train_loss: 0.00572916603414 val loss: 0.00709259393463\n",
      "Epoch [189/300], train_loss: 0.00709169333448 val loss: 0.01255645523391\n",
      "Epoch [190/300], train_loss: 0.00677261874080 val loss: 0.00975517405136\n",
      "Epoch [191/300], train_loss: 0.00654130853603 val loss: 0.01510252402901\n",
      "Epoch [192/300], train_loss: 0.00712093733840 val loss: 0.00800534895744\n",
      "Epoch [193/300], train_loss: 0.00624223921434 val loss: 0.01039676626371\n",
      "Epoch [194/300], train_loss: 0.00528533431498 val loss: 0.01685633736391\n",
      "Epoch [195/300], train_loss: 0.00438530150196 val loss: 0.01965225396270\n",
      "Epoch [196/300], train_loss: 0.00523237948886 val loss: 0.01320638781827\n",
      "Epoch [197/300], train_loss: 0.00606906680717 val loss: 0.01420497488127\n",
      "Epoch [198/300], train_loss: 0.00634384798853 val loss: 0.01084718230575\n",
      "Epoch [199/300], train_loss: 0.00452193847827 val loss: 0.01031645129681\n",
      "Epoch [200/300], train_loss: 0.00464205827516 val loss: 0.01299285940089\n",
      "Epoch [201/300], train_loss: 0.00851604722522 val loss: 0.02410291580276\n",
      "Epoch [202/300], train_loss: 0.00724133582865 val loss: 0.01378772949101\n",
      "Epoch [203/300], train_loss: 0.00466249619486 val loss: 0.01400843109392\n",
      "Epoch [204/300], train_loss: 0.00459126993219 val loss: 0.01708308042402\n",
      "Epoch [205/300], train_loss: 0.00413381448144 val loss: 0.01425442483903\n",
      "Epoch [206/300], train_loss: 0.00442764341214 val loss: 0.01581633621673\n",
      "Epoch [207/300], train_loss: 0.00631913198381 val loss: 0.01138460278000\n",
      "Epoch [208/300], train_loss: 0.00776942272890 val loss: 0.00706663225881\n",
      "Epoch [209/300], train_loss: 0.00631819931989 val loss: 0.01761444356493\n",
      "Epoch [210/300], train_loss: 0.00487205655458 val loss: 0.01944075739044\n",
      "Epoch [211/300], train_loss: 0.00492873917777 val loss: 0.01448420632611\n",
      "Epoch [212/300], train_loss: 0.00583415190340 val loss: 0.00921451398729\n",
      "Epoch [213/300], train_loss: 0.00511477383064 val loss: 0.00939513451885\n",
      "Epoch [214/300], train_loss: 0.00506246401988 val loss: 0.00941294903815\n",
      "Epoch [215/300], train_loss: 0.00436615543217 val loss: 0.00920864401592\n",
      "Epoch [216/300], train_loss: 0.00484811661462 val loss: 0.00885711488000\n",
      "Epoch [217/300], train_loss: 0.00463374249076 val loss: 0.01147174662280\n",
      "Epoch [218/300], train_loss: 0.00451074360000 val loss: 0.01271421436751\n",
      "Epoch [219/300], train_loss: 0.00466194337830 val loss: 0.01098236837101\n",
      "Epoch [220/300], train_loss: 0.00447333895992 val loss: 0.01192917359488\n",
      "Epoch [221/300], train_loss: 0.00420623384707 val loss: 0.01447703041130\n",
      "Epoch [222/300], train_loss: 0.00496298475970 val loss: 0.01118270196215\n",
      "Epoch [223/300], train_loss: 0.00553936092868 val loss: 0.01325803359699\n",
      "Epoch [224/300], train_loss: 0.00559712128597 val loss: 0.00870658324372\n",
      "Epoch [225/300], train_loss: 0.00439676099289 val loss: 0.01294236187176\n",
      "Epoch [226/300], train_loss: 0.00419499314568 val loss: 0.01283610079877\n",
      "Epoch [227/300], train_loss: 0.00508577793987 val loss: 0.01027570046588\n",
      "Epoch [228/300], train_loss: 0.00519577808796 val loss: 0.01379445746716\n",
      "Epoch [229/300], train_loss: 0.00461800048700 val loss: 0.01155545038485\n",
      "Epoch [230/300], train_loss: 0.00494726340506 val loss: 0.01106563794504\n",
      "Epoch [231/300], train_loss: 0.00413710471313 val loss: 0.01082659717657\n",
      "Epoch [232/300], train_loss: 0.00419552949161 val loss: 0.01078524666890\n",
      "Epoch [233/300], train_loss: 0.00428407695590 val loss: 0.01191189595395\n",
      "Epoch [234/300], train_loss: 0.00419864781982 val loss: 0.01259538688464\n",
      "Epoch [235/300], train_loss: 0.00409711537021 val loss: 0.01116822130684\n",
      "Epoch [236/300], train_loss: 0.00483922780190 val loss: 0.01242331221713\n",
      "Epoch [237/300], train_loss: 0.00580571801059 val loss: 0.01240250597685\n",
      "Epoch [238/300], train_loss: 0.00523478516740 val loss: 0.01489397770234\n",
      "Epoch [239/300], train_loss: 0.00455197186324 val loss: 0.01297589114741\n",
      "Epoch [240/300], train_loss: 0.00475610674084 val loss: 0.00961155315902\n",
      "Epoch [241/300], train_loss: 0.00508040995192 val loss: 0.01602254180115\n",
      "Epoch [242/300], train_loss: 0.01003550056724 val loss: 0.01221608546459\n",
      "Epoch [243/300], train_loss: 0.00665476545435 val loss: 0.01499519126244\n",
      "Epoch [244/300], train_loss: 0.00506221692665 val loss: 0.00989960530852\n",
      "Epoch [245/300], train_loss: 0.00513021374290 val loss: 0.00834394672873\n",
      "Epoch [246/300], train_loss: 0.00466189778886 val loss: 0.01037596895347\n",
      "Epoch [247/300], train_loss: 0.00443014711559 val loss: 0.01197809017483\n",
      "Epoch [248/300], train_loss: 0.00416206942592 val loss: 0.01064865641981\n",
      "Epoch [249/300], train_loss: 0.00483741899541 val loss: 0.01100982450751\n",
      "Epoch [250/300], train_loss: 0.00428937304027 val loss: 0.01192366904838\n",
      "Epoch [251/300], train_loss: 0.00440831331670 val loss: 0.01263892265787\n",
      "Epoch [252/300], train_loss: 0.00436358391958 val loss: 0.01113772988578\n",
      "Epoch [253/300], train_loss: 0.00424763510048 val loss: 0.01500511205975\n",
      "Epoch [254/300], train_loss: 0.00472994704957 val loss: 0.00796942457802\n",
      "Epoch [255/300], train_loss: 0.00426497241236 val loss: 0.00981070942038\n",
      "Epoch [256/300], train_loss: 0.00526787309312 val loss: 0.01007294416842\n",
      "Epoch [257/300], train_loss: 0.00408277819995 val loss: 0.01020158410797\n",
      "Epoch [258/300], train_loss: 0.00417211190592 val loss: 0.01462355588188\n",
      "Epoch [259/300], train_loss: 0.00413967181264 val loss: 0.01334522109634\n",
      "Epoch [260/300], train_loss: 0.00455159243703 val loss: 0.01037458926698\n",
      "Epoch [261/300], train_loss: 0.00451352184663 val loss: 0.00751005070086\n",
      "Epoch [262/300], train_loss: 0.00461587565437 val loss: 0.01122552917352\n",
      "Epoch [263/300], train_loss: 0.00483561850984 val loss: 0.00974824319241\n",
      "Epoch [264/300], train_loss: 0.00538550725918 val loss: 0.00929709664666\n",
      "Epoch [265/300], train_loss: 0.00401576528641 val loss: 0.01084424168544\n",
      "Epoch [266/300], train_loss: 0.00462200907179 val loss: 0.00978640317044\n",
      "Epoch [267/300], train_loss: 0.00435182584881 val loss: 0.01174299935034\n",
      "Epoch [268/300], train_loss: 0.00475285369355 val loss: 0.00765894325579\n",
      "Epoch [269/300], train_loss: 0.00459247988200 val loss: 0.01102136598719\n",
      "Epoch [270/300], train_loss: 0.00419119318180 val loss: 0.00946131614748\n",
      "Epoch [271/300], train_loss: 0.00457936194246 val loss: 0.01145332334110\n",
      "Epoch [272/300], train_loss: 0.00503371010863 val loss: 0.01322045139710\n",
      "Epoch [273/300], train_loss: 0.00511079921168 val loss: 0.01140300357656\n",
      "Epoch [274/300], train_loss: 0.00438829981228 val loss: 0.01376538477942\n",
      "Epoch [275/300], train_loss: 0.00463892135988 val loss: 0.01057665036771\n",
      "Epoch [276/300], train_loss: 0.00570942777182 val loss: 0.01059043626689\n",
      "Epoch [277/300], train_loss: 0.00436973290326 val loss: 0.01227688071473\n",
      "Epoch [278/300], train_loss: 0.00420227038386 val loss: 0.01407899384311\n",
      "Epoch [279/300], train_loss: 0.00451965272826 val loss: 0.01472910113969\n",
      "Epoch [280/300], train_loss: 0.00428659259770 val loss: 0.01293539369443\n",
      "Epoch [281/300], train_loss: 0.00429313755783 val loss: 0.01087009423969\n",
      "Epoch [282/300], train_loss: 0.00400159271264 val loss: 0.01262064017808\n",
      "Epoch [283/300], train_loss: 0.00427445688632 val loss: 0.00887752540196\n",
      "Epoch [284/300], train_loss: 0.00423523748074 val loss: 0.00819838312438\n",
      "Epoch [285/300], train_loss: 0.00423889431883 val loss: 0.01267140114376\n",
      "Epoch [286/300], train_loss: 0.00438502867002 val loss: 0.01050753756378\n",
      "Epoch [287/300], train_loss: 0.00395976132709 val loss: 0.01118941828665\n",
      "Epoch [288/300], train_loss: 0.00438521356409 val loss: 0.01041014786946\n",
      "Epoch [289/300], train_loss: 0.00419108334480 val loss: 0.01076513411438\n",
      "Epoch [290/300], train_loss: 0.00435429568364 val loss: 0.01175407829558\n",
      "Epoch [291/300], train_loss: 0.00390689840766 val loss: 0.01145554394073\n",
      "Epoch [292/300], train_loss: 0.00550162638469 val loss: 0.00933293743260\n",
      "Epoch [293/300], train_loss: 0.00489114902559 val loss: 0.01079640476544\n",
      "Epoch [294/300], train_loss: 0.00449997024022 val loss: 0.01212114920943\n",
      "Epoch [295/300], train_loss: 0.00475067742552 val loss: 0.01107505569234\n",
      "Epoch [296/300], train_loss: 0.00492340581310 val loss: 0.01019663185192\n",
      "Epoch [297/300], train_loss: 0.00444469198256 val loss: 0.01057673424010\n",
      "Epoch [298/300], train_loss: 0.00427595056210 val loss: 0.01023705941366\n",
      "Epoch [299/300], train_loss: 0.00441043096725 val loss: 0.01128634763073\n",
      "Epoch [300/300], train_loss: 0.00424675847856 val loss: 0.01225774300595\n",
      "Epoch [301/300], train_loss: 0.00447581603657 val loss: 0.01455492384978\n",
      "Finished training, model saved in: saved_models/best_model.pth \n"
     ]
    }
   ],
   "source": [
    "print(\"Begin training ......\")\n",
    "for epoch in range(1, num_epochs+1):  # Loop over the dataset multiple times\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "\n",
    "    # Training\n",
    "    for step, (seq, label) in enumerate(train_loader):\n",
    "        seq = seq.clone().detach().view(-1, window_size, input_size).to(device)\n",
    "        output = model(seq)\n",
    "        loss = criterion(output, label.to(device))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    \n",
    "    ave_trainloss = train_loss / len(train_loader)\n",
    "    train_loss_list.append(ave_trainloss)\n",
    "\n",
    "    # Vaildating\n",
    "    with torch.no_grad():    \n",
    "        for step, (seq, label) in enumerate(val_loader):\n",
    "            seq = seq.clone().detach().view(-1, window_size, input_size).to(device)\n",
    "            output = model(seq)\n",
    "            loss = criterion(output, label.to(device))\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    ave_valoss = val_loss / len(val_loader)\n",
    "    val_loss_list.append(ave_valoss)\n",
    "\n",
    "    if ave_valoss < loss_min:\n",
    "        loss_min = ave_valoss\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        best_model = model\n",
    "        print(\"Model saved\")\n",
    "\n",
    "    print('Epoch [{}/{}], train_loss: {:.14f} val loss: {:.14f}'.format(epoch + 1, num_epochs, ave_trainloss, ave_valoss))\n",
    "\n",
    "print(f\"Finished training, model saved in: {save_path} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = range(num_epochs)\n",
    "plt.plot(xx, train_loss_list[3:], label = \"Train\")\n",
    "plt.plot(xx, val_loss_list[3:], label = \"Val\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig(\"loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5d371ee047e82898569294ec9a92bd6efded7a8553613c637f3d7c29bad530d5"
  },
  "kernelspec": {
   "display_name": "Python 3.6.0 64-bit ('lad': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
